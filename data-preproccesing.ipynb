{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13858568,"sourceType":"datasetVersion","datasetId":8828566}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 0. Imports & configuration","metadata":{}},{"cell_type":"code","source":"import re\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T07:28:51.738534Z","iopub.execute_input":"2025-11-25T07:28:51.738840Z","iopub.status.idle":"2025-11-25T07:28:51.742993Z","shell.execute_reply.started":"2025-11-25T07:28:51.738820Z","shell.execute_reply":"2025-11-25T07:28:51.742179Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## 1. Load & concat the 10 CSVs","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom pathlib import Path\n\n# 1. set the folder that contains your 10 CSV files\nDATA_DIR = Path(\"/kaggle/input/euro-man\")  # <-- change this\n\n# 2. set a file name pattern that matches all 10 files\nPATTERN = \"euro*.csv\"  # <-- change if your names are different\n\n# 3. find all matching files\nfiles = sorted(DATA_DIR.glob(PATTERN))\n\nprint(\"Found files:\")\nfor f in files:\n    print(\"  \", f.name)\n\nprint(\"Number of files:\", len(files))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T07:28:51.753683Z","iopub.execute_input":"2025-11-25T07:28:51.753977Z","iopub.status.idle":"2025-11-25T07:28:51.786801Z","shell.execute_reply.started":"2025-11-25T07:28:51.753958Z","shell.execute_reply":"2025-11-25T07:28:51.785662Z"}},"outputs":[{"name":"stdout","text":"Found files:\n   euro000000000000.csv\n   euro000000000001.csv\n   euro000000000002.csv\n   euro000000000003.csv\n   euro000000000004.csv\n   euro000000000005.csv\n   euro000000000006.csv\n   euro000000000007.csv\n   euro000000000008.csv\n   euro000000000009.csv\nNumber of files: 10\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"dfs = []\nfor f in files:\n    print(\"Reading\", f.name)\n    df_part = pd.read_csv(f, low_memory=False)\n    dfs.append(df_part)\n\ndf_raw = pd.concat(dfs, ignore_index=True)\nprint(\"Combined rows:\", len(df_raw))\nprint(\"Columns:\", df_raw.columns.tolist())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T07:28:51.788383Z","iopub.execute_input":"2025-11-25T07:28:51.789148Z","iopub.status.idle":"2025-11-25T07:29:28.394338Z","shell.execute_reply.started":"2025-11-25T07:28:51.789121Z","shell.execute_reply":"2025-11-25T07:29:28.393621Z"}},"outputs":[{"name":"stdout","text":"Reading euro000000000000.csv\nReading euro000000000001.csv\nReading euro000000000002.csv\nReading euro000000000003.csv\nReading euro000000000004.csv\nReading euro000000000005.csv\nReading euro000000000006.csv\nReading euro000000000007.csv\nReading euro000000000008.csv\nReading euro000000000009.csv\nCombined rows: 763728\nColumns: ['publish_time', 'publish_date', 'article_url', 'source_name', 'tone_raw', 'tone_main', 'themes', 'locations', 'source_language', 'translated_to_english', 'relevance_score']\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"df_raw.to_parquet(\"euro_manu_news_all.parquet\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T07:29:28.395014Z","iopub.execute_input":"2025-11-25T07:29:28.395230Z","iopub.status.idle":"2025-11-25T07:29:36.475989Z","shell.execute_reply.started":"2025-11-25T07:29:28.395213Z","shell.execute_reply":"2025-11-25T07:29:36.475216Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import pandas as pd\n\ndf = df_raw.copy()  # if you already use df, skip this line\n\ndf[\"publish_time\"] = pd.to_datetime(df[\"publish_time\"], errors=\"coerce\", utc=True)\ndf[\"publish_date\"] = pd.to_datetime(df[\"publish_date\"], errors=\"coerce\")\n\ndf[\"month_ts\"] = (\n    df[\"publish_date\"]\n    .dt.to_period(\"M\")\n    .dt.to_timestamp(\"M\")   # month end\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T07:29:36.476738Z","iopub.execute_input":"2025-11-25T07:29:36.476920Z","iopub.status.idle":"2025-11-25T07:29:39.486964Z","shell.execute_reply.started":"2025-11-25T07:29:36.476901Z","shell.execute_reply":"2025-11-25T07:29:39.486115Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"### 直接从这里开始运行","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndf_raw = pd.read_parquet(\"euro_manu_news_all.parquet\")\nprint(\"Loaded rows:\", len(df_raw))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T07:29:39.489209Z","iopub.execute_input":"2025-11-25T07:29:39.489480Z","iopub.status.idle":"2025-11-25T07:29:46.415042Z","shell.execute_reply.started":"2025-11-25T07:29:39.489457Z","shell.execute_reply":"2025-11-25T07:29:46.413744Z"}},"outputs":[{"name":"stdout","text":"Loaded rows: 763728\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## 2. basic cleaning","metadata":{}},{"cell_type":"code","source":"import re\nimport numpy as np\nimport pandas as pd\n\ndf = df_raw.copy()\n\n# Convert dates / times\ndf[\"publish_date\"] = pd.to_datetime(df[\"publish_date\"], errors=\"coerce\")\ndf[\"publish_time\"] = pd.to_datetime(df[\"publish_time\"], errors=\"coerce\")\n\n# Numeric conversions\nfor col in [\"tone_main\", \"relevance_score\", \"translated_to_english\"]:\n    if col in df.columns:\n        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n\n# Drop rows with no URL or invalid tone\ndf = df.dropna(subset=[\"article_url\", \"tone_main\"]).copy()\n\n# Optional: restrict to time window\nmask_period = (df[\"publish_date\"] >= \"2010-01-01\") & (df[\"publish_date\"] <= \"2025-12-31\")\ndf = df.loc[mask_period].copy()\n\nprint(\"Rows after basic cleaning:\", len(df))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T07:29:46.415902Z","iopub.execute_input":"2025-11-25T07:29:46.416101Z","iopub.status.idle":"2025-11-25T07:29:49.411446Z","shell.execute_reply.started":"2025-11-25T07:29:46.416085Z","shell.execute_reply":"2025-11-25T07:29:49.410462Z"}},"outputs":[{"name":"stdout","text":"Rows after basic cleaning: 763728\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"### 去掉明显重复","metadata":{}},{"cell_type":"code","source":"df = df.sort_values([\"article_url\", \"source_name\", \"publish_time\"])\n\ndf = df.drop_duplicates(\n    subset=[\"publish_date\", \"article_url\", \"source_name\"],\n    keep=\"first\",\n).reset_index(drop=True)\n\nprint(\"Rows after dropping duplicates:\", len(df))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T07:29:49.412210Z","iopub.execute_input":"2025-11-25T07:29:49.412398Z","iopub.status.idle":"2025-11-25T07:29:52.796985Z","shell.execute_reply.started":"2025-11-25T07:29:49.412382Z","shell.execute_reply":"2025-11-25T07:29:52.796094Z"}},"outputs":[{"name":"stdout","text":"Rows after dropping duplicates: 763720\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"### 解析 GDELT 的国家信息并 explode","metadata":{}},{"cell_type":"code","source":"EURO_FIPS_TO_ISO2 = {\n    \"GM\": \"DE\", \"FR\": \"FR\", \"IT\": \"IT\", \"SP\": \"ES\", \"NL\": \"NL\",\n    \"AU\": \"AT\", \"BE\": \"BE\", \"CY\": \"CY\", \"EN\": \"EE\", \"FI\": \"FI\",\n    \"GR\": \"GR\", \"EI\": \"IE\", \"LG\": \"LV\", \"LH\": \"LT\", \"LU\": \"LU\",\n    \"MT\": \"MT\", \"PO\": \"PT\", \"LO\": \"SK\", \"SI\": \"SI\",\n}\n\nLOCATION_COUNTRY_RE = re.compile(r\"#([A-Z]{2})#\")\n\n\ndef extract_country_fips_list(loc: str) -> list[str]:\n    if not isinstance(loc, str) or not loc.strip():\n        return []\n    codes = LOCATION_COUNTRY_RE.findall(loc)\n    return list(dict.fromkeys(codes))  # unique, keep order\n\n\ndef count_locations(loc: str) -> int:\n    if not isinstance(loc, str) or not loc.strip():\n        return 0\n    return len([x for x in loc.split(\";\") if x])\n\n\ndf[\"country_fips_list\"] = df[\"locations\"].apply(extract_country_fips_list)\ndf[\"location_count\"] = df[\"locations\"].apply(count_locations)\n\ndf_exploded = df.explode(\"country_fips_list\", ignore_index=True)\ndf_exploded = df_exploded.rename(columns={\"country_fips_list\": \"country_fips\"})\n\neuro_fips_set = set(EURO_FIPS_TO_ISO2.keys())\ndf_exploded = df_exploded[df_exploded[\"country_fips\"].isin(euro_fips_set)].copy()\n\ndf_exploded[\"country_iso2\"] = df_exploded[\"country_fips\"].map(EURO_FIPS_TO_ISO2)\n\nprint(\"Rows after exploding:\", len(df_exploded))\nprint(\"Countries:\", df_exploded[\"country_iso2\"].dropna().unique())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T07:29:52.797912Z","iopub.execute_input":"2025-11-25T07:29:52.798156Z","iopub.status.idle":"2025-11-25T07:30:06.402000Z","shell.execute_reply.started":"2025-11-25T07:29:52.798135Z","shell.execute_reply":"2025-11-25T07:30:06.401022Z"}},"outputs":[{"name":"stdout","text":"Rows after exploding: 1102142\nCountries: ['GR' 'FR' 'CY' 'DE' 'AT' 'BE' 'IT' 'ES' 'SI' 'PT' 'LT' 'LU' 'LV' 'NL'\n 'EE' 'FI' 'IE' 'SK' 'MT']\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"### 时间特征","metadata":{}},{"cell_type":"code","source":"df_exploded[\"year\"] = df_exploded[\"publish_date\"].dt.year\ndf_exploded[\"month\"] = df_exploded[\"publish_date\"].dt.month\ndf_exploded[\"month_ts\"] = (\n    df_exploded[\"publish_date\"]\n    .dt.to_period(\"M\")\n    .dt.to_timestamp(\"M\")   # month end\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T07:30:06.403048Z","iopub.execute_input":"2025-11-25T07:30:06.403286Z","iopub.status.idle":"2025-11-25T07:30:06.598499Z","shell.execute_reply.started":"2025-11-25T07:30:06.403266Z","shell.execute_reply":"2025-11-25T07:30:06.597427Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"### 主题flag","metadata":{}},{"cell_type":"code","source":"def make_flag(series: pd.Series, pattern: str) -> pd.Series:\n    return series.fillna(\"\").str.contains(pattern, case=False, regex=True).astype(int)\n\n# manufacturing: anything containing \"MANUFACTURING\"\ndf_exploded[\"theme_manufacturing\"] = make_flag(\n    df_exploded[\"themes\"], r\"MANUFACTURING\"\n)\n\n# industrial production\ndf_exploded[\"theme_industrial_prod\"] = make_flag(\n    df_exploded[\"themes\"], r\"INDUSTRIAL_PRODUCTION\"\n)\n\n# energy: energy + oil/gas\ndf_exploded[\"theme_energy\"] = make_flag(\n    df_exploded[\"themes\"], r\"(ECON_ENERGY|ENV_ENERGY|ENERGY_POLICY|OIL|GAS)\"\n)\n\n# macro policy\ndf_exploded[\"theme_policy\"] = make_flag(\n    df_exploded[\"themes\"], r\"(ECON_POLICY|ECON_MONETARY_POLICY|ECON_FISCAL|PUBLIC_FINANCE)\"\n)\n\n# trade\ndf_exploded[\"theme_trade\"] = make_flag(\n    df_exploded[\"themes\"], r\"(ECON_TRADE|EXPORT|IMPORT|ECON_TARIFF)\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T07:30:06.599741Z","iopub.execute_input":"2025-11-25T07:30:06.600060Z","iopub.status.idle":"2025-11-25T07:34:59.015729Z","shell.execute_reply.started":"2025-11-25T07:30:06.600034Z","shell.execute_reply":"2025-11-25T07:34:59.014893Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_47/2845810393.py:2: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  return series.fillna(\"\").str.contains(pattern, case=False, regex=True).astype(int)\n/tmp/ipykernel_47/2845810393.py:2: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  return series.fillna(\"\").str.contains(pattern, case=False, regex=True).astype(int)\n/tmp/ipykernel_47/2845810393.py:2: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  return series.fillna(\"\").str.contains(pattern, case=False, regex=True).astype(int)\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"### 从 URL 生成 “title”，并清洗","metadata":{}},{"cell_type":"code","source":"def title_from_url(url: str) -> str:\n    if not isinstance(url, str):\n        return \"\"\n    text = re.sub(r\"^https?://\", \"\", url)\n    text = text.split(\"?\", 1)[0]\n    slug = text.strip(\"/\").split(\"/\")[-1]\n    slug = re.sub(r\"\\.(html?|php|aspx?)$\", \"\", slug, flags=re.IGNORECASE)\n    slug = slug.replace(\"-\", \" \").replace(\"_\", \" \")\n    return slug\n\n\ndef clean_title(text: str) -> str:\n    if not isinstance(text, str):\n        return \"\"\n    t = text.strip()\n    t = re.sub(r\"\\s+\", \" \", t)\n\n    t = re.sub(\n        r\"\\s*[-–—]\\s*(Reuters|Bloomberg|FT|Financial Times|The Guardian|NYTimes|New York Times|AP|AFP)$\",\n        \"\",\n        t,\n        flags=re.IGNORECASE,\n    )\n    t = re.sub(r\"\\s*\\|\\s*[^|]+$\", \"\", t)\n    return t\n\n\nif \"title\" in df_exploded.columns:\n    df_exploded[\"raw_title\"] = df_exploded[\"title\"]\nelse:\n    df_exploded[\"raw_title\"] = df_exploded[\"article_url\"].apply(title_from_url)\n\ndf_exploded[\"clean_title\"] = df_exploded[\"raw_title\"].apply(clean_title)\ndf_exploded = df_exploded[df_exploded[\"clean_title\"].str.len() >= 15].copy()\n\ndf_exploded[\"title_len_words\"] = df_exploded[\"clean_title\"].str.split().str.len()\n\nprint(\"Rows after title cleaning:\", len(df_exploded))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T07:34:59.016490Z","iopub.execute_input":"2025-11-25T07:34:59.016692Z","iopub.status.idle":"2025-11-25T07:35:12.917595Z","shell.execute_reply.started":"2025-11-25T07:34:59.016679Z","shell.execute_reply":"2025-11-25T07:35:12.916930Z"}},"outputs":[{"name":"stdout","text":"Rows after title cleaning: 911015\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"import re\nimport urllib.parse\n\ndef refine_clean_title(text: str) -> str:\n    if not isinstance(text, str):\n        return \"\"\n\n    t = text.strip()\n\n    # URL decode: turn %C2%AE into ®, etc.\n    t = urllib.parse.unquote(t)\n\n    # remove long hex-like IDs\n    t = re.sub(r\"\\b[0-9a-f]{8,}\\b\", \"\", t, flags=re.IGNORECASE)\n\n    # remove leading generic words like \"story\", \"article\"\n    t = re.sub(r\"^(story|article)\\s+\", \"\", t, flags=re.IGNORECASE)\n\n    # collapse whitespace\n    t = re.sub(r\"\\s+\", \" \", t).strip()\n    return t\n\ndf_exploded[\"clean_title\"] = df_exploded[\"clean_title\"].apply(refine_clean_title)\n\n# drop very short titles after refinement\ndf_exploded = df_exploded[df_exploded[\"clean_title\"].str.len() >= 10].copy()\n\n# normalized title for dedup\ndf_exploded[\"clean_title_norm\"] = (\n    df_exploded[\"clean_title\"].str.lower().str.strip()\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T07:35:12.918349Z","iopub.execute_input":"2025-11-25T07:35:12.918562Z","iopub.status.idle":"2025-11-25T07:35:19.094453Z","shell.execute_reply.started":"2025-11-25T07:35:12.918545Z","shell.execute_reply":"2025-11-25T07:35:19.093714Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"### 在 国家×月份 内按标题去重","metadata":{}},{"cell_type":"code","source":"df_exploded = df_exploded.sort_values(\n    [\"country_iso2\", \"month_ts\", \"publish_time\"]\n)\n\ndf_exploded = df_exploded.drop_duplicates(\n    subset=[\"country_iso2\", \"month_ts\", \"clean_title_norm\"],\n    keep=\"first\",\n).reset_index(drop=True)\n\nprint(\"Rows after dedup within country-month:\", len(df_exploded))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T07:35:19.095122Z","iopub.execute_input":"2025-11-25T07:35:19.095345Z","iopub.status.idle":"2025-11-25T07:35:20.956497Z","shell.execute_reply.started":"2025-11-25T07:35:19.095328Z","shell.execute_reply":"2025-11-25T07:35:20.955604Z"}},"outputs":[{"name":"stdout","text":"Rows after dedup within country-month: 705846\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# 1) drop rows whose themes clearly focus on entertainment people\nentertainment_pattern = r\"TAX_FNCACT_(ACTOR|ACTRESS|SINGER|MUSICIAN|COMEDIAN|FOOTBALL|BASKETBALL|BASEBALL)\"\nmask_ent_theme = df_exploded[\"themes\"].str.contains(\n    entertainment_pattern, na=False, case=False\n)\n\n# 2) drop rows from obvious entertainment sections in the URL\nentertainment_url_pattern = r\"(tvshowbiz|celebrity|entertainment|showbiz)\"\nmask_ent_url = df_exploded[\"article_url\"].str.contains(\n    entertainment_url_pattern, na=False, case=False\n)\n\ndf_exploded = df_exploded[~(mask_ent_theme | mask_ent_url)].copy()\nprint(\"Rows after dropping entertainment-like noise:\", len(df_exploded))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T07:35:20.958233Z","iopub.execute_input":"2025-11-25T07:35:20.958414Z","iopub.status.idle":"2025-11-25T07:35:36.406030Z","shell.execute_reply.started":"2025-11-25T07:35:20.958402Z","shell.execute_reply":"2025-11-25T07:35:36.405427Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_47/1665304249.py:3: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  mask_ent_theme = df_exploded[\"themes\"].str.contains(\n/tmp/ipykernel_47/1665304249.py:9: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n  mask_ent_url = df_exploded[\"article_url\"].str.contains(\n","output_type":"stream"},{"name":"stdout","text":"Rows after dropping entertainment-like noise: 665257\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"cols_to_drop = [\"source_language\", \"translated_to_english\"]\nexisting_to_drop = [c for c in cols_to_drop if c in df_exploded.columns]\n\ndf_exploded = df_exploded.drop(columns=existing_to_drop)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T07:35:36.406552Z","iopub.execute_input":"2025-11-25T07:35:36.406741Z","iopub.status.idle":"2025-11-25T07:35:36.567519Z","shell.execute_reply.started":"2025-11-25T07:35:36.406725Z","shell.execute_reply":"2025-11-25T07:35:36.566792Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"df_exploded[\"tone_main_clipped\"] = df_exploded[\"tone_main\"].clip(-10, 10)\n\ndf_exploded[\"tone_z_within_country\"] = (\n    df_exploded.groupby(\"country_iso2\")[\"tone_main_clipped\"]\n    .transform(lambda s: (s - s.mean()) / s.std(ddof=0))\n)\n\ndf_exploded.to_parquet(\"euro_manu_news_clean_exploded.parquet\", index=False)\nprint(\"Final cleaned exploded shape:\", df_exploded.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T07:35:36.568185Z","iopub.execute_input":"2025-11-25T07:35:36.568398Z","iopub.status.idle":"2025-11-25T07:35:45.914300Z","shell.execute_reply.started":"2025-11-25T07:35:36.568381Z","shell.execute_reply":"2025-11-25T07:35:45.913651Z"}},"outputs":[{"name":"stdout","text":"Final cleaned exploded shape: (665257, 26)\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# sample a small subset for quick preview in Excel / Sheets\noutput_sample = \"euro_manu_news_clean_exploded_sample_1000.csv\"\n\nsample_df = (\n    df_exploded\n    .sample(n=min(1000, len(df_exploded)), random_state=0)\n    .sort_values([\"country_iso2\", \"publish_date\"])\n)\n\nsample_df.to_csv(output_sample, index=False)\nprint(\"Saved preview sample CSV to:\", output_sample)\nprint(\"Sample shape:\", sample_df.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T07:35:45.914929Z","iopub.execute_input":"2025-11-25T07:35:45.915562Z","iopub.status.idle":"2025-11-25T07:35:46.013170Z","shell.execute_reply.started":"2025-11-25T07:35:45.915549Z","shell.execute_reply":"2025-11-25T07:35:46.012301Z"}},"outputs":[{"name":"stdout","text":"Saved preview sample CSV to: euro_manu_news_clean_exploded_sample_1000.csv\nSample shape: (1000, 26)\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"import numpy as np\n\npanel = (\n    df_exploded.groupby([\"country_iso2\", \"month_ts\"])\n      .agg(\n          news_count=(\"article_url\", \"count\"),\n          tone_mean=(\"tone_main_clipped\", \"mean\"),\n          tone_std=(\"tone_main_clipped\", \"std\"),\n          tone_min=(\"tone_main_clipped\", \"min\"),\n          tone_max=(\"tone_main_clipped\", \"max\"),\n          avg_title_len_words=(\"title_len_words\", \"mean\"),\n          avg_location_count=(\"location_count\", \"mean\"),\n          share_theme_manufacturing=(\"theme_manufacturing\", \"mean\"),\n          share_theme_industrial_prod=(\"theme_industrial_prod\", \"mean\"),\n          share_theme_energy=(\"theme_energy\", \"mean\"),\n          share_theme_policy=(\"theme_policy\", \"mean\"),\n          share_theme_trade=(\"theme_trade\", \"mean\"),\n      )\n      .reset_index()\n      .sort_values([\"country_iso2\", \"month_ts\"])\n)\n\n# derived features\npanel[\"news_log_count\"] = np.log1p(panel[\"news_count\"])\npanel[\"tone_iqr_proxy\"] = panel[\"tone_max\"] - panel[\"tone_min\"]\n\npanel.to_csv(\"panel_euro_manu_tone_enhanced.csv\", index=False)\nprint(\"Panel shape:\", panel.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T07:35:46.013909Z","iopub.execute_input":"2025-11-25T07:35:46.014138Z","iopub.status.idle":"2025-11-25T07:35:48.788316Z","shell.execute_reply.started":"2025-11-25T07:35:46.014117Z","shell.execute_reply":"2025-11-25T07:35:48.787562Z"}},"outputs":[{"name":"stdout","text":"Panel shape: (2470, 16)\n","output_type":"stream"}],"execution_count":24}]}